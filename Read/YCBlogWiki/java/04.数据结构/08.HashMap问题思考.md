#### 目录介绍
- 01.容量和装载因子
- 02.HashTable和HashMap
- 03.hashCode和equal
- 04.Key为何需要不可变
- 05.HashMap为啥要扩容
- 06.HashMap的table下标



### 01.HashMap简单使用
#### 1.1 存储数据
- 执行下面的操作时：
    ``` java
    HashMap<String, Integer> map = new HashMap<String, Integer>();
    map.put("语文", 1);
    map.put("数学", 2);
    map.put("英语", 3);
    map.put("历史", 4);
    map.put("政治", 5);
    map.put("地理", 6);
    map.put("生物", 7);
    map.put("化学", 8);
    ```


#### 1.2 查找数据
- 直接通过get()方法查找数据
    ```
    Object value = map.get(key);
    ```



#### 1.3 遍历数据
- 第一种方式：for each 遍历
    - 1: 获取所有的键对应的Set集合 ；2: 遍历Set获取每一个键
    ``` java
    // 先获取所有的键值对对象对应的Set集合
    // Set<Map.Entry<K,V>> entrySet()    重点(*****)
    Set<Entry<String,String>> entrySet = map.entrySet() ;
    // 遍历Set集合, 获取每一个元素,而每一个元素应该就是键值对对象
    for(Entry<String,String> en : entrySet) {
        // 获取键
        String key = en.getKey() ;
        // 获取值
        String value = en.getValue() ;
        // 输出
        System.out.println(key + "---" + value);
    }
    
    
    // 获取所有的键对应的Set集合
    Set<String> keys = map.keySet() ;
    // 遍历Set获取每一个键 , 根据键找出对应的值
    for(String key : keys) {
        // 根据键找出对应的值，效率相比比较低
        String value = map.get(key) ;
        // 输出
        System.out.println(key + "---" + value);
    }
    ```
- 第二种方式：使用迭代器
    ``` java
    //第二种方式，使用迭代器
    Iterator<Map.Entry<String, Integer>> iterator = map.entrySet().iterator();
    while (iterator.hasNext()){
        Map.Entry<String, Integer> next = iterator.next();
        String key = next.getKey();
        Integer value = next.getValue();
        System.out.println(key + ": " + value);
    }
    while (iterator.hasNext()){
        Map.Entry<String, Integer> next = iterator.next();
        String key = next.getKey();
        //通过key找键，效率相比比较低
        Integer value = map.get(key);
        System.out.println(key + ": " + value);
    }
    ```
- 运行结果是
    ```
    > 政治: 5  
    > 生物: 7  
    > 历史: 4  
    > 数学: 2  
    > 化学: 8  
    > 语文: 1  
    > 英语: 3  
    > 地理: 6
    ```



#### 1.4 结构图
- 下面是一个大致的结构，希望我们对HashMap的结构有一个感性的认识：  
    - ![](https://cloud.githubusercontent.com/assets/1736354/6957741/0c039b1c-d933-11e4-8c1e-7558a8766272.png "hashmap")





### 02.HashMap特点
#### 2.1 官方说明
- 在官方文档中是这样描述HashMap的：
    - Hash table based**implementation of the Map interface**. This implementation provides all of the optional map operations, and permits null values and the null key. (The HashMap class is roughly equivalent to Hashtable, except that it is**unsynchronized**and**permits nulls**.) This class makes no guarantees as to the order of the map; in particular, it does not guarantee that the order will remain constant over time.
- 几个关键的信息：[博客](https://github.com/yangchong211/YCBlogs)
    - 基于Map接口实现、允许null键/值、是非同步(这点很重要，多线程注意)、不保证有序(比如插入的顺序)、也不保证序不随时间变化。
- 如何理解允许null键/值？
    - 允许插入最多一条`key`为`null`的记录，允许插入多条`value`为`null`的记录。
- 如何理解不保证有序？
    - HashMap 不保证元素顺序，根据需要该容器可能会对元素重新哈希，元素的顺序也会被重新打散，因此在不同时间段迭代同一个 HashMap 的顺序可能会不同。
- 如何理解非同步？
    - HashMap 非线程安全，即任一时刻有多个线程同时写 HashMap 的话可能会导致数据的不一致



#### 2.2 原理简单说明
- HashMap基于哈希思想，实现对数据的读写。
    - put存储对象时，我们将K/V传给put方法时，它调用hashCode计算hash从而得到bucket位置，进一步存储，HashMap会根据当前bucket的占用情况自动调整容量(超过`Load Facotr`则resize为原来的2倍)。
    - get获取对象时，我们将K传给get，它调用hashCode计算hash从而得到bucket位置，并进一步调用equals()方法确定键值对。
- HashMap使用链表来解决碰撞问题，当发生碰撞了，对象将会储存在链表的下一个节点中。 
    - 如果发生碰撞的时候，Hashmap通过链表将产生碰撞冲突的元素组织起来，在Java 8中，如果一个bucket中碰撞冲突的元素超过某个限制(默认是8)，则使用红黑树来替换链表，从而提高速度。


#### 2.3 底层实现思考
- 底层是用什么实现的？
    - HashMap 实际上是**数组+链表+红黑树**的结合体……
- 为什么要使用链表？
    - 才有hash算法，如果出现碰撞，则需要把碰撞的数据添加到链表上面。
- 为什么要使用红黑树？
    - 因为链表查找效率低，因此 JDK1.8 开始 HashMap 通过使用红黑树来提高元素查找效率。



### 03.HashMap线程问题
- HashMap是非线程安全的，那么测试一下，先看下测试代码
    ``` java
    private HashMap map = new HashMap();
    private void test(){
        Thread t1 = new Thread() {
            @Override
            public void run() {
                for (int i = 0; i < 100; i++) {
                    map.put(new Integer(i), i);
                }
                LogUtils.d("yc-----执行结束----t1");
            }
        };
    
        Thread t2 = new Thread() {
            @Override
            public void run() {
                for (int i = 0; i < 100; i++) {
                    map.put(new Integer(i), i);
                }
                LogUtils.d("yc-----执行结束----t2");
            }
        };
    
        Thread t3 = new Thread() {
            @Override
            public void run() {
                for (int i = 0; i < 100; i++) {
                    map.put(new Integer(i), i);
                }
                LogUtils.d("yc-----执行结束----t2");
            }
        };
    
        Thread t4 = new Thread() {
            @Override
            public void run() {
                for (int i = 0; i < 100; i++) {
                    map.get(new Integer(i));
                }
                LogUtils.d("yc-----执行结束----t2");
            }
        };
    
        Thread t5 = new Thread() {
            @Override
            public void run() {
                for (int i = 0; i < 100; i++) {
                    map.get(new Integer(i));
                }
                LogUtils.d("yc-----执行结束----t2");
            }
        };
    
        Thread t6 = new Thread() {
            @Override
            public void run() {
                for (int i = 0; i < 100; i++) {
                    map.get(new Integer(i));
                }
                LogUtils.d("yc-----执行结束----t2");
            }
        };
    
    
        t1.start();
        t2.start();
        t3.start();
        t4.start();
        t5.start();
        t6.start();
    }
    ```
- 就是启了6个线程，不断的往一个非线程安全的HashMap中put/get内容，put的内容很简单，key和value都是从0自增的整数（这个put的内容做的并不好，以致于后来干扰了我分析问题的思路）。对HashMap做并发写操作，我原以为只不过会产生脏数据的情况，但反复运行这个程序，会出现线程t1、t2被卡住的情况，多数情况下是一个线程被卡住另一个成功结束，偶尔会6个线程都被卡住。[博客](https://github.com/yangchong211/YCBlogs)
    - 多线程下直接使用 ConcurrentHashMap，解决了这个问题。
    - CPU利用率过高一般是因为出现了出现了死循环，导致部分线程一直运行，占用cpu时间。问题原因就是HashMap是非线程安全的，多个线程put的时候造成了某个key值Entry key List的死循环，问题就这么产生了。
    - 当另外一个线程get 这个Entry List 死循环的key的时候，这个get也会一直执行。最后结果是越来越多的线程死循环，最后导致卡住。我们一般认为HashMap重复插入某个值的时候，会覆盖之前的值，这个没错。但是对于多线程访问的时候，由于其内部实现机制(在多线程环境且未作同步的情况下，对同一个HashMap做put操作可能导致两个或以上线程同时做rehash动作，就可能导致循环键表出现，一旦出现线程将无法终止，持续占用CPU，导致CPU使用率居高不下)，就可能出现安全问题了。





### 04.测试HashMap效率
- 需求：测试下不同的初始化大小以及 key 值的 HashCode 值的分布情况的不同对 HashMap 效率的影响
- 测试初始化大小对 HashMap 的性能影响！！！
    - 首先来定义作为 Key 的类，`hashCode()` 方法直接返回其包含的属性 value。
    ``` java
    public class Key {
        private int value;
        public Key(int value) {
            this.value = value;
        }

        @Override
        public boolean equals(Object o) {
            if (this == o) {
                return true;
            }
            if (o == null || getClass() != o.getClass()) {
                return false;
            }
            Key key = (Key) o;
            return value == key.value;
        }

        @Override
        public int hashCode() {
            return value;
        }
    }
    
    private static final int MAX_KEY = 20000;
    private static final Key[] KEYS = new Key[MAX_KEY];

    private void testHashMap(){
        for (int i = 0; i < MAX_KEY; i++) {
            KEYS[i] = new Key(i);
        }
        //测试
        for (int i = 20; i <= MAX_KEY; i *= 10) {
            test(i);
        }
    }

    private static void test(int size) {
        long startTime = System.currentTimeMillis();
        Map<Key, Integer> map = new HashMap<>(size);
        for (int i = 0; i < MAX_KEY; i++) {
            map.put(KEYS[i], i);
        }
        long endTime = System.currentTimeMillis();
        System.out.println("yc---初始化大小是：" + size + " , 所用时间：" + (endTime - startTime) + "毫秒");
    }
    ```
    - 初始化大小从 100 到 100000 之间以 10 倍的倍数递增，向 HashMap 存入同等数据量的数据，观察不同 HashMap 存入数据消耗的总时间。例子中，各个Key对象之间的哈希码值各不相同，所以键值对在哈希桶数组中的分布可以说是很均匀的了，此时主要影响性能的就是扩容机制了，由上图可以看出各个初始化大小对 HashMap 的性能影响还是很大的。[博客](https://github.com/yangchong211/YCBlogs)
    ``` java
    2019-05-07 18:41:48.899 1522-1522/? I/System.out: yc---初始化大小是：20 , 所用时间：20毫秒
    2019-05-07 18:41:48.906 1522-1522/? I/System.out: yc---初始化大小是：200 , 所用时间：5毫秒
    2019-05-07 18:41:48.906 1522-1522/? I/System.out: yc---初始化大小是：2000 , 所用时间：0毫秒
    ```
- 然后测试Key对象之间频繁发生哈希冲突时HashMap的性能
    - 令 Key 类的 `hashCode()` 方法固定返回 100，则每个键值对在存入 HashMap 时，一定会发生哈希冲突。可以看到此时存入同等数据量的数据所用时间呈几何数增长了，此时主要影响性能的点就在于对哈希冲突的处理
    ``` java
    2019-05-07 18:40:11.213 1003-1003/com.ycbjie.ycexpandview I/System.out: yc---初始化大小是：20 , 所用时间：281毫秒
    2019-05-07 18:40:11.459 1003-1003/com.ycbjie.ycexpandview I/System.out: yc---初始化大小是：200 , 所用时间：246毫秒
    2019-05-07 18:40:11.673 1003-1003/com.ycbjie.ycexpandview I/System.out: yc---初始化大小是：2000 , 所用时间：213毫秒
    ```



### 05.HashMap性能分析
- 查找key的时候，时间复杂度是 O(1)，同时它也花费了更多的内存空间。
    - 缺点:
    - 自动装箱的存在意味着每一次插入都会有额外的对象创建。这跟垃圾回收机制一样也会影响到内存的利用。
    - HashMap.Entry 对象本身是一层额外需要被创建以及被垃圾回收的对象。
    - “桶” 在 HashMap 每次被压缩或扩容的时候都会被重新安排。这个操作会随着对象数量的增长而变得开销极大。
- 对于查找一个key时，HashMap会发生什么 ?[博客](https://github.com/yangchong211/YCBlogs)
    - 键的哈希值先被计算出来
    - 在 mHashes[] 数组中二分查找此哈希值。这表明查找的时间复杂度增加到了 O(logN)。
    - 一旦得到了哈希值所对应的索引 index，键值对中的键就存储在 mArray[2index] ，值存储在 mArray[2index+1]。
    - 这里的时间复杂度从 O(1) 上升到 O(logN)，但是内存效率提升了。当我们在 100 左右的数据量范围内尝试时，没有耗时的问题，察觉不到时间上的差异，但我们应用的内存效率获得了提高。
- Android中推荐使用ArrayMap或者SparseArray替代HashMap
    - 在Android中，当涉及到快速响应的应用时，内存至关重要，因为持续地分发和释放内存会出发垃圾回收机制，这会拖慢应用运行。
    - 垃圾回收时间段内，应用程序是不会运行的，最终应用使用上就显得卡顿。
    - ArrayMap 使用2个数组。它的对象实例内部有用来存储对象的 Object[] mArray 和 存储哈希值的 int[] mHashes。当插入一个键值对时：
        - 键/值被自动装箱。
        - 键对象被插入到 mArray[] 数组中的下一个空闲位置。
        - 值对象也会被插入到 mArray[] 数组中与键对象相邻的位置。
        - 键的哈希值会被计算出来并被插入到 mHashes[] 数组中的下一个空闲位置。



- HashMap 添加元素优化
    - 初始化完成后，HashMap 就可以使用 put() 方法添加键值对了。从下面源码可以看出，当程序将一个 key-value 对添加到 HashMap 中，程序首先会根据该 key 的 hashCode() 返回值，再通过 hash() 方法计算出 hash 值，再通过 putVal 方法中的 (n - 1) & hash 决定该 Node 的存储位置。
    ```
    public V put(K key, V value) {
        return putVal(hash(key), key, value, false, true);
    }

    static final int hash(Object key) {
        int h;
        return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
    }

    if ((tab = table) == null || (n = tab.length) == 0)
        n = (tab = resize()).length;
        //通过putVal方法中的(n - 1) & hash决定该Node的存储位置
        if ((p = tab[i = (n - 1) & hash]) == null)
            tab[i] = newNode(hash, key, value, null);
    ```
    - 如果你不太清楚 hash() 以及 (n-1)&hash 的算法，就请你看下面的详述。我们先来了解下 hash() 方法中的算法。如果我们没有使用 hash() 方法计算 hashCode，而是直接使用对象的 hashCode 值，会出现什么问题呢？
        - 假设要添加两个对象 a 和 b，如果数组长度是 16，这时对象 a 和 b 通过公式 (n - 1) & hash 运算，也就是 (16-1)＆a.hashCode 和 (16-1)＆b.hashCode，15 的二进制为 0000000000000000000000000001111，假设对象 A 的 hashCode 为 1000010001110001000001111000000，对象 B 的 hashCode 为 0111011100111000101000010100000，你会发现上述与运算结果都是 0。这样的哈希结果就太让人失望了，很明显不是一个好的哈希算法。
        - 但如果我们将 hashCode 值右移 16 位（h >>> 16 代表无符号右移 16 位），也就是取 int 类型的一半，刚好可以将该二进制数对半切开，并且使用位异或运算（如果两个数对应的位置相反，则结果为 1，反之为 0），这样的话，就能避免上面的情况发生。这就是 hash() 方法的具体实现方式。简而言之，就是尽量打乱 hashCode 真正参与运算的低 16 位。
        - 再来解释下 (n - 1) & hash 是怎么设计的，这里的 n 代表哈希表的长度，哈希表习惯将长度设置为 2 的 n 次方，这样恰好可以保证 (n - 1) & hash 的计算得到的索引值总是位于 table 数组的索引之内。例如：hash=15，n=16 时，结果为 15；hash=17，n=16 时，结果为 1。
- HashMap 获取元素优化
    - 当 HashMap 中只存在数组，而数组中没有 Node 链表时，是 HashMap 查询数据性能最好的时候。一旦发生大量的哈希冲突，就会产生 Node 链表，这个时候每次查询元素都可能遍历 Node 链表，从而降低查询数据的性能。
    - 特别是在链表长度过长的情况下，性能将明显降低，红黑树的使用很好地解决了这个问题，使得查询的平均复杂度降低到了 O(log(n))，链表越长，使用黑红树替换后的查询效率提升就越明显。
    - 在编码中也可以优化 HashMap 的性能，例如，重写 key 值的 hashCode() 方法，降低哈希冲突，从而减少链表的产生，高效利用哈希表，达到提高性能的效果。
- HashMap 扩容优化
    - HashMap 也是数组类型的数据结构，所以一样存在扩容的情况。在 JDK1.7 中，HashMap 整个扩容过程就是分别取出数组元素，一般该元素是最后一个放入链表中的元素，然后遍历以该元素为头的单向链表元素，依据每个被遍历元素的 hash 值计算其在新数组中的下标，然后进行交换。这样的扩容方式会将原来哈希冲突的单向链表尾部变成扩容后单向链表的头部。
    - 而在 JDK 1.8 中，HashMap 对扩容操作做了优化。由于扩容数组的长度是 2 倍关系，所以对于假设初始 tableSize = 4 要扩容到 8 来说就是 0100 到 1000 的变化（左移一位就是 2 倍），在扩容中只用判断原来的 hash 值和左移动的一位（newtable 的值）按位与操作是 0 或 1 就行，0 的话索引不变，1 的话索引变成原索引加上扩容前数组。
    - 之所以能通过这种“与运算“来重新分配索引，是因为 hash 值本来就是随机的，而 hash 按位与上 newTable 得到的 0（扩容前的索引位置）和 1（扩容前索引位置加上扩容前数组长度的数值索引处）就是随机的，所以扩容的过程就能把之前哈希冲突的元素再随机分布到不同的索引中去。
- 为什么HashMap最大容量是2的30次方?
    - 因为int的最大值是2的31-1,而hashMap最大容量不可以超过int的最大值,所有是2的30次方,




### 01.容量和装载因子
- 在HashMap中有两个很重要的参数，容量(Capacity)和负载因子(Load factor)
    - 简单的说，Capacity就是bucket的大小，Loadfactor就是bucket填满程度的最大比例。
    - 如果对迭代性能要求很高的话，不要把`capacity`设置过大，也不要把`load factor`设置过小。
    - 当bucket中的entries的数目大于`capacity*load factor`时就需要调整bucket的大小为当前的2倍。
- 什么是装载因子
    - 装载因子用于规定数组在自动扩容之前可以数据占有其容量的最高比例，即当数据量占有数组的容量达到这个比例后，数组将自动扩容。
    - 装载因子衡量的是一个散列表的空间的使用程度，装载因子越大表示散列表的装填程度越高，反之愈小。因此如果装载因子越大，则对空间的利用程度更高，相对应的是查找效率的降低。
    - 如果装载因子太小，那么数组的数据将过于稀疏，对空间的利用率低，官方默认的装载因子为0.75，是平衡空间利用率和运行效率两者之后的结果。
    - 如果在实际情况中，内存空间较多而对时间效率要求很高，可以选择降低装载因子的值；如果内存空间紧张而对时间效率要求不高，则可以选择提高装载因子的值。[博客](https://github.com/yangchong211/YCBlogs)
    - 此外，即使装载因子和哈希算法设计得再合理，也不免会出现由于哈希冲突导致链表长度过长的情况，这将严重影响 HashMap 的性能。为了优化性能，从 JDK1.8 开始引入了红黑树，当链表长度超出 TREEIFY_THRESHOLD 规定的值时，链表就会被转换为红黑树，利用红黑树快速增删改查的特点以提高 HashMap 的性能。
    ```
    //序列化ID
    private static final long serialVersionUID = 362498820763181265L;
    
    //哈希桶数组的默认容量
    static final int DEFAULT_INITIAL_CAPACITY = 1 << 4;
    
    //网上很多文章都说这个值是哈希桶数组能够达到的最大容量，其实这样说并不准确
    //从 resize() 方法的扩容机制可以看出来，HashMap 每次扩容都是将数组的现有容量增大一倍
    //如果现有容量已大于或等于 MAXIMUM_CAPACITY ，则不允许再次扩容
    //否则即使此次扩容会导致容量超出 MAXIMUM_CAPACITY ，那也是允许的
    static final int MAXIMUM_CAPACITY = 1 << 30;
    
    //装载因子的默认值
    //装载因子用于规定数组在自动扩容之前可以数据占有其容量的最高比例，即当数据量占有数组的容量达到这个比例后，数组将自动扩容
    //装载因子衡量的是一个散列表的空间的使用程度，负载因子越大表示散列表的装填程度越高，反之愈小
    //对于使用链表的散列表来说，查找一个元素的平均时间是O(1+a)，因此如果负载因子越大，则对空间的利用程度更高，相对应的是查找效率的降低
    //如果负载因子太小，那么数组的数据将过于稀疏，对空间的利用率低
    //官方默认的负载因子为0.75，是平衡空间利用率和运行效率两者之后的结果
    static final float DEFAULT_LOAD_FACTOR = 0.75f;
    
    //为了提高效率，当链表的长度超出这个值时，就将链表转换为红黑树
    static final int TREEIFY_THRESHOLD = 8;
    ```
- HashMap的大小超过了负载因子(`load factor`)定义的容量，怎么办？
    - 如果超过了负载因子(默认**0.75**)，则会重新resize一个原来长度两倍的HashMap，并且重新调用hash方法。




### 02.HashTable和HashMap
- HashTable和HashMap初始化与增长方式
    - 初始化时：
        - HashTable在不指定容量的情况下的默认容量为11，且不要求底层数组的容量一定要为2的整数次幂；HashMap默认容量为16，且要求容量一定为2的整数次幂。
    - 扩容时：[博客](https://github.com/yangchong211/YCBlogs)
        - Hashtable将容量变为原来的2倍加1；HashMap扩容将容量变为原来的2倍。
- HashTable和HashMap线程安全性
    - HashTable其方法函数都是同步的（采用synchronized修饰），不会出现两个线程同时对数据进行操作的情况，因此保证了线程安全性。也正因为如此，在多线程运行环境下效率表现非常低下。因为当一个线程访问HashTable的同步方法时，其他线程也访问同步方法就会进入阻塞状态。比如当一个线程在添加数据时候，另外一个线程即使执行获取其他数据的操作也必须被阻塞，大大降低了程序的运行效率，在新版本中已被废弃，不推荐使用。
    - HashMap不支持线程的同步，即任一时刻可以有多个线程同时写HashMap;可能会导致数据的不一致。如果需要同步（1）可以用Collections的synchronizedMap方法；（2）使用ConcurrentHashMap类，相较于HashTable锁住的是对象整体，ConcurrentHashMap基于lock实现锁分段技术。首先将Map存放的数据分成一段一段的存储方式，然后给每一段数据分配一把锁，当一个线程占用锁访问其中一个段的数据时，其他段的数据也能被其他线程访问。ConcurrentHashMap不仅保证了多线程运行环境下的数据访问安全性，而且性能上有长足的提升。




### 03.hashCode和equal
- get和put的中的equals()和hashCode()的都有什么作用？
    - 通过对key的hashCode()进行hashing，并计算下标( `(n-1) & hash`)，从而获得buckets的位置。
    - 如果产生碰撞，则利用key.equals()方法去链表或树中去查找对应的节点




### 04.Key为何需要不可变
- 为什么HashMap中String、Integer这样的包装类适合作为K？
    - String、Integer等包装类的特性能够保证Hash值的不可更改性和计算准确性，能够有效的减少Hash碰撞的几率
        - 都是final类型，即不可变性，保证key的不可更改性，不会存在获取hash值不同的情况
        - 内部已重写了equals()、hashCode()等方法，遵守了HashMap内部的规范（不清楚可以去上面看看putValue的过程），不容易出现Hash值计算错误的情况
        - 如果对象在创建后它的哈希值发生了变化，则Map对象很可能就定位不到映射的位置。
- 想要让自己的Object作为K应该怎么办呢？[博客](https://github.com/yangchong211/YCBlogs)
    - 重写hashCode()和equals()方法
        - 重写hashCode()是因为需要计算存储数据的存储位置，需要注意不要试图从散列码计算中排除掉一个对象的关键部分来提高性能，这样虽然能更快但可能会导致更多的Hash碰撞；
        - 重写equals()方法，需要遵守自反性、对称性、传递性、一致性以及对于任何非null的引用值x，x.equals(null)必须返回false的这几个特性，目的是为了保证key在哈希表中的唯一性；
- 总结
    - 采用合适的equals()和hashCode()方法的话，将会减少碰撞的发生，提高效率。不可变性使得能够缓存不同键的hashcode，这将提高整个获取对象的速度，使用String，Interger这样的wrapper类作为键是非常好的选择。





### 05.HashMap为啥要扩容
- HashMap是为啥要扩容
    - 当链表数组的容量超过初始容量*加载因子（默认0.75）时，再散列将链表数组扩大2倍，把原链表数组的搬移到新的数组中。
    - 为什么需要使用加载因子？为什么需要扩容呢？因为如果填充比很大，说明利用的空间很多，如果一直不进行扩容的话，链表就会越来越长，这样查找的效率很低，扩容之后，将原来链表数组的每一个链表分成奇偶两个子链表分别挂在新链表数组的散列位置，这样就减少了每个链表的长度，增加查找效率。
- 重新调整HashMap大小存在什么问题吗？[博客](https://github.com/yangchong211/YCBlogs)
    - 当多线程的情况下，可能产生条件竞争。当重新调整HashMap大小的时候，确实存在条件竞争，因为如果两个线程都发现HashMap需要重新调整大小了，它们会同时试着调整大小。
    - 在调整大小的过程中，存储在链表中的元素的次序会反过来，因为移动到新的bucket位置的时候，HashMap并不会将元素放在链表的尾部，而是放在头部，这是为了避免尾部遍历(tail traversing)。如果条件竞争发生了，那么就死循环了。




### 06.HashMap的table下标
- 它是用自己的hash方法确定下标
    - HashMap自己实现了自己的hash()方法，通过两次扰动使得它自己的哈希值高低位自行进行异或运算，降低哈希碰撞概率也使得数据分布更平均；
    - 在保证数组长度为2的幂次方的时候，使用hash()运算之后的值与运算（&）（数组长度 - 1）来获取数组下标的方式进行存储，这样一来是比取余操作更加有效率，二来也是因为只有当数组长度为2的幂次方时，h&(length-1)才等价于h%length，三来解决了“哈希值与数组大小范围不匹配”的问题；
- 不直接使用hashCode()处理后的哈希值
    - hashCode()方法返回的是int整数类型，其范围为-(2^31)~(2^31-1)，约有40亿个映射空间，而HashMap的容量范围是在16（初始化默认值）~2 ^ 30，HashMap通常情况下是取不到最大值的，并且设备上也难以提供这么多的存储空间，从而导致通过hashCode()计算出的哈希值可能不在数组大小范围内，进而无法匹配存储位置；
- 为什么数组长度要保证为2的幂次方呢？
    - 只有当数组长度为2的幂次方时，h&(length-1)才等价于h%length，即实现了key的定位，2的幂次方也可以减少冲突次数，提高HashMap的查询效率；[博客](https://github.com/yangchong211/YCBlogs)
    - 如果 length 为 2 的次幂 则 length-1 转化为二进制必定是 11111……的形式，在于 h 的二进制与操作效率会非常的快，而且空间不浪费；如果 length 不是 2 的次幂，比如 length 为 15，则 length - 1 为 14，对应的二进制为 1110，在于 h 与操作，最后一位都为 0 ，而 0001，0011，0101，1001，1011，0111，1101 这几个位置永远都不能存放元素了，空间浪费相当大，更糟的是这种情况中，数组可以使用的位置比数组长度小了很多，这意味着进一步增加了碰撞的几率，减慢了查询的效率！这样就会造成空间的浪费。

- 优秀文章
- https://juejin.cn/post/7077363148281348126


### 其他介绍
#### 01.关于博客汇总链接
- 1.[技术博客汇总](https://www.jianshu.com/p/614cb839182c)
- 2.[开源项目汇总](https://blog.csdn.net/m0_37700275/article/details/80863574)
- 3.[生活博客汇总](https://blog.csdn.net/m0_37700275/article/details/79832978)
- 4.[喜马拉雅音频汇总](https://www.jianshu.com/p/f665de16d1eb)
- 5.[其他汇总](https://www.jianshu.com/p/53017c3fc75d)



#### 02.关于我的博客
- github：https://github.com/yangchong211
- 知乎：https://www.zhihu.com/people/yczbj/activities
- 简书：http://www.jianshu.com/u/b7b2c6ed9284
- csdn：http://my.csdn.net/m0_37700275
- 喜马拉雅听书：http://www.ximalaya.com/zhubo/71989305/
- 开源中国：https://my.oschina.net/zbj1618/blog
- 泡在网上的日子：http://www.jcodecraeer.com/member/content_list.php?channelid=1
- 邮箱：yangchong211@163.com
- 阿里云博客：https://yq.aliyun.com/users/article?spm=5176.100- 239.headeruserinfo.3.dT4bcV
- segmentfault头条：https://segmentfault.com/u/xiangjianyu/articles
- 掘金：https://juejin.im/user/5939433efe88c2006afa0c6e



